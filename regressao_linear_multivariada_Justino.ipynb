{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPhxjqpg6zw4HEVacX0udu4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mestrie/Regress-o_linear_atv3/blob/main/regressao_linear_multivariada_Justino.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##features_normalize"
      ],
      "metadata": {
        "id": "xHti1_5NloMP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9oNdNBjafWf6"
      },
      "outputs": [],
      "source": [
        "# Functions/feature_normalize.py\n",
        "\"\"\"\n",
        "@file features_normalizes.py\n",
        "@brief Funções para normalização de features em datasets.\n",
        "@details Este módulo contém funções para normalizar as features de um dataset\n",
        "          utilizando diferentes abordagens, como média e desvio padrão, ou\n",
        "          mínimo e máximo.\n",
        "@author Your Name <your.email@example.com>\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def features_normalize_by_std(X):\n",
        "    \"\"\"\n",
        "    Normaliza as features de um dataset para média zero e desvio padrão unitário.\n",
        "    Matematicamente, a formula utilizada é:\n",
        "        X_norm = (X - mu) / sigma\n",
        "    onde:\n",
        "        - X é a matriz de entrada (m x n) onde m é o número de amostras e n é o número de features.\n",
        "        - mu é o vetor de médias (1 x n) de cada feature.\n",
        "        - sigma é o vetor de desvios padrão (1 x n) de cada feature.\n",
        "\n",
        "    :param (ndarray) X: Matriz de entrada onde cada linha é uma amostra e cada coluna é uma feature.\n",
        "    :return (tuple): Uma tripla contendo:\n",
        "        - X_norm (ndarray): Matriz normalizada.\n",
        "        - mu (ndarray): Vetor com as médias de cada feature.\n",
        "        - sigma (ndarray): Vetor com os desvios padrão de cada feature.\n",
        "    \"\"\"\n",
        "    # Calcula a média de cada feature (coluna)\n",
        "    mu = np.mean(X, axis=0)\n",
        "\n",
        "    # Calcula o desvio padrão de cada feature (coluna)\n",
        "    sigma = np.std(X, axis=0)\n",
        "\n",
        "    # Normaliza as features subtraindo a média e dividindo pelo desvio padrão\n",
        "    # Verifica se sigma é zero (o que indicaria que todas as amostras têm o mesmo valor na feature)\n",
        "    # Se sigma for zero, substitui por 1 para evitar divisão por zero\n",
        "    # Isso garante que a normalização não cause problemas numéricos\n",
        "    # e que a feature não seja eliminada do conjunto de dados\n",
        "    if np.any(sigma == 0):\n",
        "        sigma[sigma == 0] = 1\n",
        "\n",
        "    # Normaliza as features\n",
        "    X_norm = (X - mu) / sigma\n",
        "\n",
        "    return X_norm, mu, sigma\n",
        "\n",
        "\n",
        "def features_normalizes_by_min_max(X):\n",
        "    \"\"\"\n",
        "    Normaliza as features de um dataset para o intervalo [0, 1] utilizando o mínimo e o máximo.\n",
        "    Matematicamente, a formula utilizada é:\n",
        "        X_norm = (X - min) / (max - min)\n",
        "    onde:\n",
        "        - X é a matriz de entrada (m x n) onde m é o número de amostras e n é o número de features.\n",
        "        - min é o vetor de mínimos (1 x n) de cada feature.\n",
        "        - max é o vetor de máximos (1 x n) de cada feature.\n",
        "\n",
        "    :param (ndarray) X: Matriz de entrada onde cada linha é uma amostra e cada coluna é uma feature.\n",
        "    :return (tuple): Uma tupla contendo:\n",
        "        - X_norm (ndarray): Matriz normalizada.\n",
        "        - min (ndarray): Vetor com os valores mínimos de cada feature.\n",
        "        - max (ndarray): Vetor com os valores máximos de cada feature.\n",
        "    \"\"\"\n",
        "    # Calcula o mínimo de cada feature (coluna)\n",
        "    min = np.min(X, axis=0)\n",
        "\n",
        "    # Calcula o máximo de cada feature (coluna)\n",
        "    max = np.max(X, axis=0)\n",
        "\n",
        "    # Normaliza as features subtraindo o mínimo e dividindo pela diferença entre máximo e mínimo\n",
        "    # Verifica se max - min é zero (o que indicaria que todas as amostras têm o mesmo valor na feature)\n",
        "    # Se max - min for zero, substitui por 1 para evitar divisão por zero\n",
        "    # Isso garante que a normalização não cause problemas numéricos\n",
        "    # e que a feature não seja eliminada do conjunto de dados\n",
        "    range_ = max - min\n",
        "    range_[range_ == 0] = 1\n",
        "\n",
        "    # Normaliza as features\n",
        "    X_norm = (X - min) / range_\n",
        "\n",
        "    return X_norm, min, max\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##compute_cost_multi"
      ],
      "metadata": {
        "id": "mtaSgQA-ywSS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Functions/compute_cost_multi.py\n",
        "\"\"\"\n",
        "@file compute_cost_multi.py\n",
        "@brief Computes the cost for multivariate linear regression.\n",
        "@details Este módulo contém uma função para calcular o custo de um modelo de regressão linear\n",
        "          multivariada utilizando a função de custo de erro quadrático médio.\n",
        "@author Your Name <your.email@example.com>\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def compute_cost_multi(X, y, theta):\n",
        "    \"\"\"\n",
        "    Calcula o custo para regressão linear multivariada.\n",
        "\n",
        "    A função de custo é definida como:\n",
        "        J(θ) = (1 / (2m)) * (Xθ - y)ᵀ (Xθ - y)\n",
        "\n",
        "    :param (ndarray) X: Matriz de features incluindo o termo de intercepto (shape: m × n+1).\n",
        "    :param (ndarray) y: Vetor de valores alvo (shape: m,).\n",
        "    :param (ndarray) theta: Vetor de parâmetros (shape: n+1,).\n",
        "    :return (float): Valor do custo calculado.\n",
        "    \"\"\"\n",
        "    # get the number of training examples\n",
        "    m = len(y)\n",
        "\n",
        "    # compute the predictions using the linear model by formula h(θ) = X @ θ\n",
        "    # where @ is the matrix multiplication operator\n",
        "    predictions = X.dot(theta)\n",
        "\n",
        "    # compute the error vector between predictions and actual values\n",
        "    # The error is the difference between the predicted values and the actual values\n",
        "    errors = predictions - y\n",
        "\n",
        "    # compute the cost as the mean squared error cost function using the formula in the docstring\n",
        "    cost = (1 / (2 * m)) * np.dot(errors.T, errors)\n",
        "\n",
        "    return cost\n"
      ],
      "metadata": {
        "id": "PErGDnMAyzH2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##gradient_descent_multi.py"
      ],
      "metadata": {
        "id": "ritd-PhH_AI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Functions/gradient_descent_multi.py\n",
        "\"\"\"\n",
        "@file gradient_descent_multi.py\n",
        "@brief Performs gradient descent for multivariate regression.\n",
        "@details Este módulo contém uma função para executar o gradiente descendente\n",
        "          para regressão linear multivariada, atualizando os parâmetros θ\n",
        "          iterativamente para minimizar a função de custo.\n",
        "@author Your Name <your.email@example.com>\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "def gradient_descent_multi(X, y, theta, alpha, num_iters):\n",
        "    \"\"\"\n",
        "    Executa o gradiente descendente para aprender os parâmetros θ.\n",
        "\n",
        "    Atualiza θ realizando num_iters passos de gradiente com taxa de aprendizado α usando a fórmula:\n",
        "        θ := θ - α * (1/m) * (Xᵀ * (Xθ - y))\n",
        "    onde:\n",
        "        - θ é o vetor de parâmetros (n+1,).\n",
        "        - m é o número de amostras.\n",
        "        - X é a matriz de features (m × n+1).\n",
        "        - y é o vetor de valores alvo (m,).\n",
        "        - α é a taxa de aprendizado.\n",
        "\n",
        "    :param (ndarray) X: Matriz de features com termo de bias (shape: m × n+1).\n",
        "    :param (ndarray) y: Vetor de valores alvo (shape: m,).\n",
        "    :param (ndarray) theta: Vetor de parâmetros iniciais (shape: n+1,).\n",
        "    :param (float) alpha: Taxa de aprendizado.\n",
        "    :param (int) num_iters: Número de iterações.\n",
        "    :return (tuple): Uma tupla com 2 elementos contendo:\n",
        "        - theta (ndarray): Parâmetros aprendidos (shape: n+1,).\n",
        "        - J_history (ndarray): Custo em cada iteração (shape: num_iters,).\n",
        "    \"\"\"\n",
        "    # obtenha o número de exemplos de treinamento\n",
        "    m = len(y)\n",
        "\n",
        "    # inicialize o vetor de custo para armazenar o custo em cada iteração com 0s\n",
        "    # O vetor J_history armazena o custo em cada iteração do gradiente descendente\n",
        "    J_history = np.zeros(num_iters)\n",
        "\n",
        "    # loop para atualizar os parâmetros θ usando o gradiente descendente\n",
        "    # O loop itera num_iters vezes, atualizando os parâmetros θ a cada iteração\n",
        "    for i in range(num_iters):\n",
        "        # calcule o erro entre as previsões e os valores reais\n",
        "        # O erro é a diferença entre os valores previstos e os valores reais\n",
        "        error = X.dot(theta) - y\n",
        "\n",
        "        # calcule o gradiente para atualizar os parâmetros θ\n",
        "        # O gradiente é a derivada da função de custo em relação aos parâmetros θ\n",
        "        # O gradiente é um vetor que aponta na direção de maior aumento da função de custo\n",
        "        # O gradiente é calculado como a média do erro multiplicado pela matriz de features transposta\n",
        "        # O gradiente é um vetor que representa a direção e a magnitude da mudança necessária\n",
        "        gradient = (1 / m) * X.T.dot(error)\n",
        "\n",
        "        # A atualização dos parâmetros θ é feita usando a fórmula do gradiente descendente\n",
        "        # A taxa de aprendizado α controla o tamanho do passo na direção do gradiente\n",
        "        # A atualização dos parâmetros θ é feita na direção oposta ao gradiente\n",
        "        # para minimizar a função de custo\n",
        "        # Atualize os parâmetros θ subtraindo pelo gradiente multiplicado pela taxa de aprendizado α\n",
        "        theta = theta - alpha * gradient\n",
        "\n",
        "        # Agora Calcule o custo atual e armazene-o no vetor J_history\n",
        "        # O custo é calculado usando a função de custo definida na função compute_cost_multi\n",
        "        # O custo é uma medida de quão bem o modelo se ajusta aos dados de treinamento\n",
        "        # Quando o custo é baixo, significa que o modelo está fazendo previsões precisas\n",
        "        J_history[i] = compute_cost_multi(X, y, theta)\n",
        "\n",
        "    return theta, J_history\n",
        "\n",
        "\n",
        "def gradient_descent_multi_with_history(X, y, theta, alpha, num_iters):\n",
        "    \"\"\"\n",
        "    Executa o gradiente descendente para aprender os parâmetros θ.\n",
        "\n",
        "    Atualiza θ realizando num_iters passos de gradiente com taxa de aprendizado α usando a fórmula:\n",
        "        θ := θ - α * (1/m) * (Xᵀ * (Xθ - y))\n",
        "    onde:\n",
        "        - θ é o vetor de parâmetros (n+1,).\n",
        "        - m é o número de amostras.\n",
        "        - X é a matriz de features (m × n+1).\n",
        "        - y é o vetor de valores alvo (m,).\n",
        "        - α é a taxa de aprendizado.\n",
        "\n",
        "    :param (ndarray) X: Matriz de features com termo de bias (shape: m × n+1).\n",
        "    :param (ndarray) y: Vetor de valores alvo (shape: m,).\n",
        "    :param (ndarray) theta: Vetor de parâmetros iniciais (shape: n+1,).\n",
        "    :param (float) alpha: Taxa de aprendizado.\n",
        "    :param (int) num_iters: Número de iterações.\n",
        "    :return (tuple): Uma tupla com 3 elementos contendo:\n",
        "        - theta (ndarray): Parâmetros aprendidos (shape: n+1,).\n",
        "        - J_history (ndarray): Custo em cada iteração (shape: num_iters,).\n",
        "        - theta_history (ndarray): Histórico dos parâmetros θ em cada iteração (shape: num_iters + 1 × n+1).\n",
        "    \"\"\"\n",
        "    # obtenha o número de exemplos de treinamento\n",
        "    m = len(y)\n",
        "\n",
        "    # obtenha o número de parâmetros\n",
        "    n = theta.shape[0]\n",
        "\n",
        "    # inicialize o vetor de custo para armazenar o custo em cada iteração com 0s\n",
        "    # O vetor J_history armazena o custo em cada iteração do gradiente descendente\n",
        "    J_history = np.zeros(num_iters)\n",
        "\n",
        "    # inicialize o vetor de histórico de parâmetros para armazenar os parâmetros em cada iteração com 0s\n",
        "    # O vetor theta_history armazena os parâmetros em cada iteração do gradiente descendente\n",
        "    # Deve ter o mesmo número de linhas que o número de iterações+1 e o mesmo número de colunas que o número de parâmetros\n",
        "    theta_history = np.zeros((num_iters + 1, n))\n",
        "\n",
        "    # armazene os parâmetros iniciais no vetor de histórico de parâmetros\n",
        "    # Faça uma cópia dos parâmetros iniciais para evitar modificações indesejadas\n",
        "    # Agora você sabe porque o vetor theta_history tem num_iters+1 linhas\n",
        "    theta_history[0] = theta.copy()\n",
        "\n",
        "    # loop para atualizar os parâmetros θ usando o gradiente descendente\n",
        "    # O loop itera num_iters vezes, atualizando os parâmetros θ a cada iteração\n",
        "    for i in range(num_iters):\n",
        "        # calcule o erro entre as previsões e os valores reais\n",
        "        # O erro é a diferença entre os valores previstos e os valores reais\n",
        "        error = X.dot(theta) - y\n",
        "\n",
        "        # calcule o gradiente para atualizar os parâmetros θ\n",
        "        # O gradiente é a derivada da função de custo em relação aos parâmetros θ\n",
        "        # O gradiente é um vetor que aponta na direção de maior aumento da função de custo\n",
        "        # O gradiente é calculado como a média do erro multiplicado pela matriz de features transposta\n",
        "        # O gradiente é um vetor que representa a direção e a magnitude da mudança necessária\n",
        "        gradient = (1 / m) * X.T.dot(error)\n",
        "\n",
        "        # A atualização dos parâmetros θ é feita usando a fórmula do gradiente descendente\n",
        "        # A taxa de aprendizado α controla o tamanho do passo na direção do gradiente\n",
        "        # A atualização dos parâmetros θ é feita na direção oposta ao gradiente\n",
        "        # para minimizar a função de custo\n",
        "        # Atualize os parâmetros θ subtraindo pelo gradiente multiplicado pela taxa de aprendizado α\n",
        "        theta = theta - alpha * gradient\n",
        "\n",
        "        # Agora Calcule o custo atual e armazene-o no vetor J_history\n",
        "        # O custo é calculado usando a função de custo definida na função compute_cost_multi\n",
        "        # O custo é uma medida de quão bem o modelo se ajusta aos dados de treinamento\n",
        "        # Quando o custo é baixo, significa que o modelo está fazendo previsões precisas\n",
        "        J_history[i] = compute_cost_multi(X, y, theta)\n",
        "\n",
        "        # Agora aqui é que vem a mágica, a diferença entre essa função e a anterior\n",
        "        # Armazene os parâmetros θ atuais no vetor de histórico de parâmetros\n",
        "        # Isso é útil para visualizar como os parâmetros mudam ao longo do tempo\n",
        "        # Faça uma cópia dos parâmetros theta atuais para evitar modificações indesejadas\n",
        "        # Use o i+1 para armazenar os parâmetros na linha correta do vetor de histórico de parâmetros\n",
        "        theta_history[i + 1] = theta.copy()\n",
        "\n",
        "    return theta, J_history, theta_history\n"
      ],
      "metadata": {
        "id": "cXeoOpuO_Hj0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##normal_eqn.py"
      ],
      "metadata": {
        "id": "IAN9-UPMAlws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Functions/normal_eqn.py\n",
        "\"\"\"\n",
        "@file normal_eqn.py\n",
        "@brief Calcula os parâmetros θ usando a Equação Normal.\n",
        "@details Este módulo contém uma função para calcular os parâmetros de um modelo\n",
        "          de regressão linear utilizando a equação normal.\n",
        "@author Your Name <your.email@example.com>\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def normal_eqn(X, y):\n",
        "    \"\"\"\n",
        "    Resolve os parâmetros θ utilizando a equação normal.\n",
        "\n",
        "    A equação normal é definida como:\n",
        "        θ = (XᵀX)⁻¹ Xᵀ y\n",
        "\n",
        "    :param (ndarray) X: Matriz de features com bias, onde cada linha é uma amostra\n",
        "                        e cada coluna é uma feature (shape: m × n+1).\n",
        "    :param (ndarray) y: Vetor de valores alvo (shape: m,).\n",
        "    :return (ndarray): Vetor de parâmetros θ (shape: n+1,).\n",
        "    \"\"\"\n",
        "    # Calcula os parâmetros θ utilizando a equação normal\n",
        "    # A equação normal é uma solução fechada para o problema de regressão linear\n",
        "    # que minimiza a soma dos erros quadráticos entre as previsões e os valores reais\n",
        "    # Implemente aqui a equação normal descrita na docstring. Use a função np.linalg.pinv\n",
        "    # para calcular a pseudo-inversa de uma matriz, que é útil quando a matriz não é quadrada\n",
        "    # ou não é invertível.\n",
        "    # A pseudo-inversa é uma generalização da inversa de uma matriz e pode ser usada para resolver\n",
        "    # sistemas de equações lineares que não têm uma solução única ou que são mal condicionados.\n",
        "    theta = np.linalg.pinv(X.T @ X) @ X.T @ y\n",
        "    return theta\n"
      ],
      "metadata": {
        "id": "WpN_CV9wB88i"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}